{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../input')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Aspect Based Sentiment Analysis on Car Reviews\n",
    "## Taking Toyota Cars as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_rev = pd.read_csv('../input/Scrapped_Car_Reviews_Toyota.csv',engine='python',index_col=False)\n",
    "toy_rev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Combining the review title and review body for the text corpus****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_rev['review']=toy_rev['Review_Title']+toy_rev['Review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Using spaCy for dependency parsing which forms the crux of aspect extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.load('en_core_web_lg', parse=True, tag=True, entity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-2fb8e436bec7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-2fb8e436bec7>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    python -m spacy download en_core_web_lg\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **Using spaCy's awesome displacy module to show the dependency relations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'Great car and has long range'\n",
    "nlp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'Great car and has long range'\n",
    "doc = nlp(txt)\n",
    "spacy.displacy.render(doc,style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**from https://nlp.stanford.edu/software/dependencies_manual.pdf**\n",
    "### AMOD - adjectival modifier\n",
    "#### An adjectival modifier of a Noun is any adjectival phrase that serves to modify the meaning of the Noun\n",
    "### ex - 'Great <--amod-- Car', 'Long <--amod-- range'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'Drives well has great handling'\n",
    "doc = nlp(txt)\n",
    "spacy.displacy.render(doc,style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADVMOD - adverb modifier\n",
    "#### An adverb modifier of a word is a (non-clausal) adverb or adverb-headed phrase that serves to modify\n",
    "#### the meaning of the word\n",
    "### ex - 'Drives --advmod--> well'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt =  \"wonderful to drive the camry \"\n",
    "doc = nlp(txt)\n",
    "spacy.displacy.render(doc,style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XCOMP -  open clausal complement\n",
    "#### An open clausal complement (xcomp) of a verb or an adjective is a predicative or clausal complement without its own subject\n",
    "### ex - 'wonderful --xcomp--> drive'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt =  \"not wonderful to drive the camry \"\n",
    "doc = nlp(txt)\n",
    "spacy.displacy.render(doc,style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEG - self explanatory\n",
    "### ex - not <--neg-- wonderful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPOUND WORDS\n",
    "#### Generally from a review standpoint, compound words often do not offer us sentiments per se, hence my code looks for possible compound word pairs and then checks with the aspect words extracted if it can add more detail to the extracted aspects - ex Outstanding passenger van gives *more context* than Outstanding van (which is what my code would have extracted without the compound word search) while the compound word search will identify passenger van as a compound word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = ['Chevy','chevy','Ford','ford','Nissan','nissan','Honda','honda','Chevrolet','chevrolet','Volkswagen','volkswagen','benz','Benz','Mercedes','mercedes','subaru','Subaru','VW']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reason for using competitor name list is to remove potential misleading aspects-sentiments, since we are interested to acquire aspect info about Toyota and not any other brand. This is because a reviewer might be comparing a Benz saying it has superior handling when compared to the car the person is reviewing and this can lead to misclassifications******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in tqdm(range(len(toy_rev['review']))):\n",
    "count=0\n",
    "print(toy_rev['review'][0])\n",
    "if len(str(toy_rev['review'][0])) != 0:\n",
    "        lines = str(toy_rev['review'][0]).replace('*',' ').replace('-',' ').replace('so ',' ').replace('be ',' ').replace('are ',' ').replace('just ',' ').replace('get ','').replace('were ',' ').replace('When ','').replace('when ','').replace('again ',' ').replace('where ','').replace('how ',' ').replace('has ',' ').replace('Here ',' ').replace('here ',' ').replace('now ',' ').replace('see ',' ').replace('why ',' ').split('.')       \n",
    "        print(len(lines))\n",
    "        for line in lines:\n",
    "            #print('line',line)\n",
    "            enem_list = []\n",
    "            #print(len(competitors))\n",
    "            for eny in competitors:\n",
    "                enem = re.search(eny,line)\n",
    "                count=count+1\n",
    "                #print(count)\n",
    "                #print('enem',enem)\n",
    "                if enem is not None:\n",
    "                    #print('efwe',enem.group())    \n",
    "                    enem_list.append(enem.group())\n",
    "            if len(enem_list)==0:\n",
    "                doc = nlp(line)\n",
    "                print(\"doc\",doc)\n",
    "                str1=''\n",
    "                str2=''        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_terms = []\n",
    "comp_terms = []\n",
    "easpect_terms = []\n",
    "ecomp_terms = []\n",
    "enemy = []\n",
    "for x in tqdm(range(len(toy_rev['review']))):\n",
    "    amod_pairs = []\n",
    "    advmod_pairs = []\n",
    "    compound_pairs = []\n",
    "    xcomp_pairs = []\n",
    "    neg_pairs = []\n",
    "    eamod_pairs = []\n",
    "    eadvmod_pairs = []\n",
    "    ecompound_pairs = []\n",
    "    eneg_pairs = []\n",
    "    excomp_pairs = []\n",
    "    enemlist = []\n",
    "    if len(str(toy_rev['review'][x])) != 0:\n",
    "        lines = str(toy_rev['review'][x]).replace('*',' ').replace('-',' ').replace('so ',' ').replace('be ',' ').replace('are ',' ').replace('just ',' ').replace('get ','').replace('were ',' ').replace('When ','').replace('when ','').replace('again ',' ').replace('where ','').replace('how ',' ').replace('has ',' ').replace('Here ',' ').replace('here ',' ').replace('now ',' ').replace('see ',' ').replace('why ',' ').split('.')       \n",
    "        for line in lines:\n",
    "            enem_list = []\n",
    "            for eny in competitors:\n",
    "                enem = re.search(eny,line)\n",
    "                if enem is not None:\n",
    "                    enem_list.append(enem.group())\n",
    "            if len(enem_list)==0:\n",
    "                doc = nlp(line)\n",
    "                str1=''\n",
    "                str2=''\n",
    "                for token in doc:\n",
    "                    if token.pos_ is 'NOUN':\n",
    "                        for j in token.lefts:\n",
    "                            if j.dep_ == 'compound':\n",
    "                                compound_pairs.append((j.text+' '+token.text,token.text))\n",
    "                            if j.dep_ is 'amod' and j.pos_ is 'ADJ': #primary condition\n",
    "                                str1 = j.text+' '+token.text\n",
    "                                amod_pairs.append(j.text+' '+token.text)\n",
    "                                for k in j.lefts:\n",
    "                                    if k.dep_ is 'advmod': #secondary condition to get adjective of adjectives\n",
    "                                        str2 = k.text+' '+j.text+' '+token.text\n",
    "                                        amod_pairs.append(k.text+' '+j.text+' '+token.text)\n",
    "                                mtch = re.search(re.escape(str1),re.escape(str2))\n",
    "                                if mtch is not None:\n",
    "                                    amod_pairs.remove(str1)\n",
    "                    if token.pos_ is 'VERB':\n",
    "                        for j in token.lefts:\n",
    "                            if j.dep_ is 'advmod' and j.pos_ is 'ADV':\n",
    "                                advmod_pairs.append(j.text+' '+token.text)\n",
    "                            if j.dep_ is 'neg' and j.pos_ is 'ADV':\n",
    "                                neg_pairs.append(j.text+' '+token.text)\n",
    "                        for j in token.rights:\n",
    "                            if j.dep_ is 'advmod'and j.pos_ is 'ADV':\n",
    "                                advmod_pairs.append(token.text+' '+j.text)\n",
    "                    if token.pos_ is 'ADJ':\n",
    "                        for j,h in zip(token.rights,token.lefts):\n",
    "                            if j.dep_ is 'xcomp' and h.dep_ is not 'neg':\n",
    "                                for k in j.lefts:\n",
    "                                    if k.dep_ is 'aux':\n",
    "                                        xcomp_pairs.append(token.text+' '+k.text+' '+j.text)\n",
    "                            elif j.dep_ is 'xcomp' and h.dep_ is 'neg':\n",
    "                                if k.dep_ is 'aux':\n",
    "                                        neg_pairs.append(h.text +' '+token.text+' '+k.text+' '+j.text)\n",
    "            \n",
    "            else:\n",
    "                enemlist.append(enem_list)\n",
    "                doc = nlp(line)\n",
    "                str1=''\n",
    "                str2=''\n",
    "                for token in doc:\n",
    "                    if token.pos_ is 'NOUN':\n",
    "                        for j in token.lefts:\n",
    "                            if j.dep_ == 'compound':\n",
    "                                ecompound_pairs.append((j.text+' '+token.text,token.text))\n",
    "                            if j.dep_ is 'amod' and j.pos_ is 'ADJ': #primary condition\n",
    "                                str1 = j.text+' '+token.text\n",
    "                                eamod_pairs.append(j.text+' '+token.text)\n",
    "                                for k in j.lefts:\n",
    "                                    if k.dep_ is 'advmod': #secondary condition to get adjective of adjectives\n",
    "                                        str2 = k.text+' '+j.text+' '+token.text\n",
    "                                        eamod_pairs.append(k.text+' '+j.text+' '+token.text)\n",
    "                                mtch = re.search(re.escape(str1),re.escape(str2))\n",
    "                                if mtch is not None:\n",
    "                                    eamod_pairs.remove(str1)\n",
    "                    if token.pos_ is 'VERB':\n",
    "                        for j in token.lefts:\n",
    "                            if j.dep_ is 'advmod' and j.pos_ is 'ADV':\n",
    "                                eadvmod_pairs.append(j.text+' '+token.text)\n",
    "                            if j.dep_ is 'neg' and j.pos_ is 'ADV':\n",
    "                                eneg_pairs.append(j.text+' '+token.text)\n",
    "                        for j in token.rights:\n",
    "                            if j.dep_ is 'advmod'and j.pos_ is 'ADV':\n",
    "                                eadvmod_pairs.append(token.text+' '+j.text)\n",
    "                    if token.pos_ is 'ADJ':\n",
    "                        for j in token.rights:\n",
    "                            if j.dep_ is 'xcomp':\n",
    "                                for k in j.lefts:\n",
    "                                    if k.dep_ is 'aux':\n",
    "                                        excomp_pairs.append(token.text+' '+k.text+' '+j.text)\n",
    "        pairs = list(set(amod_pairs+advmod_pairs+neg_pairs+xcomp_pairs))\n",
    "        epairs = list(set(eamod_pairs+eadvmod_pairs+eneg_pairs+excomp_pairs))\n",
    "        for i in range(len(pairs)):\n",
    "            if len(compound_pairs)!=0:\n",
    "                for comp in compound_pairs:\n",
    "                    mtch = re.search(re.escape(comp[1]),re.escape(pairs[i]))\n",
    "                    if mtch is not None:\n",
    "                        pairs[i] = pairs[i].replace(mtch.group(),comp[0])\n",
    "        for i in range(len(epairs)):\n",
    "            if len(ecompound_pairs)!=0:\n",
    "                for comp in ecompound_pairs:\n",
    "                    mtch = re.search(re.escape(comp[1]),re.escape(epairs[i]))\n",
    "                    if mtch is not None:\n",
    "                        epairs[i] = epairs[i].replace(mtch.group(),comp[0])\n",
    "            \n",
    "    aspect_terms.append(pairs)\n",
    "    comp_terms.append(compound_pairs)\n",
    "    easpect_terms.append(epairs)\n",
    "    ecomp_terms.append(ecompound_pairs)\n",
    "    enemy.append(enemlist)\n",
    "toy_rev['compound_nouns'] = comp_terms\n",
    "toy_rev['aspect_keywords'] = aspect_terms\n",
    "toy_rev['competition'] = enemy\n",
    "toy_rev['competition_comp_nouns'] = ecomp_terms\n",
    "toy_rev['competition_aspects'] = easpect_terms\n",
    "toy_rev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We use vaderSentiment for sentiment analysis because of it's speed and simplicity. It offers 3 types of polarity -  positive, negative and neutral. As a result we can filter all aspects which have high neutral scores hence minimizing errors caused due to wrong extraction of aspects and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sentiment = []\n",
    "for i in range(len(toy_rev)):\n",
    "    score_dict={'pos':0,'neg':0,'neu':0}\n",
    "    if len(toy_rev['aspect_keywords'][i])!=0: \n",
    "        for aspects in toy_rev['aspect_keywords'][i]:\n",
    "            sent = analyser.polarity_scores(aspects)\n",
    "            score_dict['neg'] += sent['neg']\n",
    "            score_dict['pos'] += sent['pos']\n",
    "        #score_dict['neu'] += sent['neu']\n",
    "        sentiment.append(max(score_dict.items(), key=operator.itemgetter(1))[0])\n",
    "    else:\n",
    "        sentiment.append('NaN')\n",
    "toy_rev['sentiment'] = sentiment\n",
    "toy_rev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_sent = []\n",
    "for sent in toy_rev['sentiment']:\n",
    "    if sent is 'NaN':\n",
    "        int_sent.append('NaN')\n",
    "    elif sent is 'pos':\n",
    "        int_sent.append('1')\n",
    "    else:\n",
    "        int_sent.append('0')\n",
    "toy_rev['int_sent'] = int_sent\n",
    "toy_rev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we have arbitarily taken ratings greater than 3 as positive and everything else as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "pos = []\n",
    "for i in range(len(toy_rev)):\n",
    "    if not math.isnan(toy_rev['Rating'][i]):\n",
    "        if int(toy_rev['Rating'][i])>3:\n",
    "            pos.append('1')\n",
    "        else:\n",
    "            pos.append('0')\n",
    "    else:\n",
    "        pos.append('0')\n",
    "toy_rev['Positive Review'] = pos\n",
    "toy_rev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'sent':toy_rev['Positive Review'],'sent_pred':toy_rev['int_sent']}\n",
    "metric_df = pd.DataFrame(data=d)\n",
    "metric_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metric_df.sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing NaN values in the sentiment predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = metric_df[metric_df.sent_pred != 'NaN']\n",
    "len(metric_df.sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,auc,f1_score,recall_score,precision_score\n",
    "print('accuracy')\n",
    "print(accuracy_score(metric_df.sent, metric_df.sent_pred))\n",
    "print('f1 score')\n",
    "print(f1_score(metric_df.sent, metric_df.sent_pred,pos_label='1'))\n",
    "print('recall')\n",
    "print(recall_score(metric_df.sent, metric_df.sent_pred,pos_label='1'))\n",
    "print('precision')\n",
    "print(precision_score(metric_df.sent, metric_df.sent_pred,pos_label='1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible improvements that can be made\n",
    "*  Tricky situation of removing stopwords to reduce unwanted extractions of non-aspects but this can also affect spaCy's dependency parsing. Same goes with noun chunk merging as well. If someone can think of a better way to remove stopwords and still retain spaCy's dependency goodness it can greatly improve the accuracy\n",
    "\n",
    "* This is not a ML task per se since we do more of parsing than ML. Although Bi-Directional LSTM have been very good at ABSA tasks in the past, unlike semeval tasks we do not have a fixed topic for our aspects to fall into. If someone can use the parsing aspect of the code to implement BLSTM in this case, that would be great\n",
    "\n",
    "* better alternatives to vaderSentiment if available (unsupervised/ semi-supervised methods might be better here I think)\n",
    "\n",
    "* The very definition of aspects can be a bit vague at times hence we do not have a valid metric to measure the aspect extraction's accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P.S This is my first Kaggle Kernel and I am fairly new to python programming as well, hence my non usage of list comprehensions and functions might be evident. I highly encourage everyone to fork my code and add your own twists to increase the accuracy of both aspect extractions and sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
